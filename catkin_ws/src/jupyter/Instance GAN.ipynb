{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import VGG\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, bins):\n",
    "    '''\n",
    "    Convert tensor x to one-hot tensor\n",
    "    '''\n",
    "    x = x.numpy()\n",
    "    idxs = np.digitize(x, bins, right=True)\n",
    "    idxs = idxs.reshape(-1,1)\n",
    "    z = torch.zeros(len(x), len(bins)+1).scatter_(1, torch.tensor(idxs), 1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=4, stride=2, padding=1, dropout=0.0, bn=True):\n",
    "        super(Conv, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(out_size))\n",
    "        #layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class DeConv(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=4, stride=2, padding=1, dropout=0.0):\n",
    "        super(DeConv, self).__init__()\n",
    "        layers = [  nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "                    nn.BatchNorm2d(out_size),\n",
    "                    nn.ReLU(inplace=True)]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    '''def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_encode = 64\n",
    "n_gen = 64\n",
    "latent_s = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_encode = 32\n",
    "        self.conv1 = Conv(in_channels, n_encode, 5, 2, 1)\n",
    "        self.conv2 = Conv(n_encode, 2*n_encode, 5, 2, 1)\n",
    "        self.conv3 = Conv(2*n_encode, 4*n_encode, 5, 2, 1)\n",
    "        self.conv4 = Conv(4*n_encode, 8*n_encode, 5, 2, 1, dropout=0.5)\n",
    "        self.fc1 = nn.Sequential(nn.Linear(8*n_encode*8*8, latent_s))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(4096, 100))\n",
    "    def forward(self, x):\n",
    "        c1 = F.relu(self.conv1(x))\n",
    "        c2 = F.relu(self.conv2(c1))\n",
    "        c3 = F.relu(self.conv3(c2))\n",
    "        c4 = F.relu(self.conv4(c3))\n",
    "        c = c4.view(-1, 8*n_encode*8*8)\n",
    "        f1 = self.fc1(c)\n",
    "        out = f1\n",
    "        '''f = d5.view(d5.size(0), -1)\n",
    "        l1 = F.relu(self.linear1(f))\n",
    "        l2 = F.relu(self.linear2(l1))\n",
    "        y = l2.unsqueeze(-1).unsqueeze(-1)'''\n",
    "        '''print(\"x : \", x.shape)\n",
    "        print(\"c1: \", c1.shape)\n",
    "        print(\"c2: \", c2.shape)\n",
    "        print(\"c3: \", c3.shape)\n",
    "        print(\"c4: \", c4.shape)\n",
    "        print(\"c : \", c.shape)\n",
    "        print(\"f1: \", f1.shape)\n",
    "        print(\"out: \", out.shape)'''\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.zeros([1, 3, 128, 128])\n",
    "x.shape\n",
    "e = Encoder()\n",
    "y = e(x)\n",
    "print(y.shape)\n",
    "\n",
    "cls = torch.tensor(2).unsqueeze(0)\n",
    "cls = one_hot(cls, [0, 1])\n",
    "print(cls, cls.shape)\n",
    "\n",
    "z = torch.cat((y, cls), 1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=2):\n",
    "        super(Generator, self).__init__()\n",
    "        #in_channels, out_channels, kernel_size, stride, padding\n",
    "        self.up1 = DeConv(8*n_gen, 4*n_gen, 4, 2, 0, 0.0)\n",
    "        self.up2 = DeConv(4*n_gen, 2*n_gen, 4, 2, 1, 0.0)\n",
    "        self.up3 = DeConv(2*n_gen, 1*n_gen, 4, 2, 1, 0.0)\n",
    "        self.up4 = DeConv(1*n_gen, out_channels, 4, 2, 1, 0.5)\n",
    "        self.fc1 = nn.Sequential(nn.Linear(latent_s + 3, 8*8*n_gen*8))\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        z = torch.tensor(z).unsqueeze(0).cpu()\n",
    "        z = one_hot(z, [0, 1])\n",
    "        if cuda:\n",
    "            z = z.cuda()\n",
    "        x = torch.cat((x, z), 1)\n",
    "        fc = self.fc1(x).view(-1, 8*n_gen,8,8)\n",
    "        u1 = F.relu(self.up1(fc))\n",
    "        u2 = F.relu(self.up2(u1))\n",
    "        u3 = F.relu(self.up3(u2))\n",
    "        u4 = F.relu(self.up4(u3))\n",
    "        y = u4\n",
    "        '''print(\"x : \", x.shape)\n",
    "        print(\"fc1: \", fc.shape)\n",
    "        print(\"u1: \", u1.shape)\n",
    "        print(\"u2: \", u2.shape)\n",
    "        print(\"u3: \", u3.shape)\n",
    "        print(\"u4: \", u4.shape)\n",
    "        print(\"u5: \", u5.shape)\n",
    "        print(\"y: \", y.shape)'''\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G = Generator()\n",
    "#print(z.shape)\n",
    "g = G(y, 3)\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, y_size, conv_dim=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        self.y_size = y_size\n",
    "        self.conv1 = Conv(2 + 3 + y_size, conv_dim, 4)\n",
    "        self.conv2 = Conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = Conv(conv_dim*2, conv_dim*4, 4)\n",
    "        self.conv4 = Conv(conv_dim*4, conv_dim*8, 4, bn=False)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(conv_dim*8*8*8, 1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "            \n",
    "    def forward(self, img_A, img_B, z):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        z = torch.tensor(z).unsqueeze(0).cpu()\n",
    "        z = one_hot(z, [0, 1])\n",
    "        if cuda:\n",
    "            z = z.cuda()\n",
    "        z = z.view(-1,z.size()[-1],1,1)\n",
    "        z = z.expand(-1,-1,img_input.size()[-2], img_input.size()[-1])\n",
    "        x = torch.cat((img_input, z), 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(1, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D = Discriminator(3)\n",
    "\n",
    "img = torch.zeros([1, 2, 128, 128])\n",
    "d = D(img, img, 2)\n",
    "print(d.shape)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root = \"/media/arg_ws3/5E703E3A703E18EB/data/subt_all/\", mode='train'):\n",
    "        transforms_ = [ transforms.Resize((128, 128), Image.BICUBIC),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\n",
    "\n",
    "        self.transform_ = transforms.Compose(transforms_)\n",
    "        self.root = root\n",
    "        self.files = []\n",
    "        self.obj_class = ['bb_extinguisher', 'bb_drill', 'bb_backpack']\n",
    "        for line in open(os.path.join(root, mode + '.txt')):\n",
    "            self.files.append(line.strip())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = index % len(self.files)\n",
    "        ann_path = self.root + 'Annotations/' + self.files[idx] + '.xml'\n",
    "        mask_path = self.root + 'mask/' + self.files[idx] + '.png'\n",
    "        rgb_path = self.root + 'image/' + self.files[idx] + '.jpg'\n",
    "\n",
    "        bbx = self.get_ann(ann_path)\n",
    "\n",
    "        img_mask = Image.open(mask_path).crop(bbx).resize((128, 128), Image.ANTIALIAS)\n",
    "        img_rgb = Image.open(rgb_path).crop(bbx).resize((128, 128), Image.ANTIALIAS)\n",
    "\n",
    "        img_mask = img_mask.convert('L')\n",
    "\n",
    "        img_mask = np.array(img_mask)\n",
    "\n",
    "        img_rgb = np.array(img_rgb)\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            img_mask = np.array(img_mask)[::-1, :]\n",
    "            img_rgb = np.array(img_rgb)[::-1, :]\n",
    "\n",
    "        # 1 channel to 2 channel classifier (one hot encoder)\n",
    "        n_class = 2\n",
    "        h, w = img_mask.shape\n",
    "        target = torch.zeros(n_class, h, w)\n",
    "        clss = img_mask[img_mask!=0]\n",
    "        cls_counts = np.bincount(clss)\n",
    "        label = np.argmax(cls_counts)\n",
    "        img_mask[img_mask!=0] = 1 # 255 to 1\n",
    "        img_mask = torch.from_numpy(img_mask.copy()).long()\n",
    "        for i in range(n_class):\n",
    "            target[i][img_mask == i] = 1\n",
    "\n",
    "        img_rgb = self.transform_(Image.fromarray(img_rgb))\n",
    "\n",
    "        return {'A': target, 'B': img_rgb, 'C': label}\n",
    "\n",
    "    def get_ann(self, ann_path):\n",
    "        target = ET.parse(ann_path).getroot()\n",
    "        res = []\n",
    "        for obj in target.iter('object'):\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            if name not in self.obj_class:\n",
    "                continue\n",
    "            bbox = obj.find('bndbox')\n",
    "            if bbox is not None:\n",
    "                pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "                bndbox = []\n",
    "                for i, pt in enumerate(pts):\n",
    "                    cur_pt = int(bbox.find(pt).text) - 1\n",
    "                    # scale height or width\n",
    "                    #cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n",
    "                    bndbox.append(cur_pt)\n",
    "                res += [bndbox]  # [xmin, ymin, xmax, ymax]\n",
    "            else: # For LabelMe tool\n",
    "                polygons = obj.find('polygon')\n",
    "                x = []\n",
    "                y = []\n",
    "                bndbox = []\n",
    "                for polygon in polygons.iter('pt'):\n",
    "                    # scale height or width\n",
    "                    x.append(int(polygon.find('x').text))\n",
    "                    y.append(int(polygon.find('y').text))\n",
    "                bndbox.append(min(x))\n",
    "                bndbox.append(min(y))\n",
    "                bndbox.append(max(x))\n",
    "                bndbox.append(max(y))\n",
    "                res += [bndbox] # [xmin, ymin, xmax, ymax]\n",
    "        return res[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ImageDataset(\"/media/arg_ws3/5E703E3A703E18EB/data/subt_all/\"),\n",
    "                        batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(ImageDataset(\"/media/arg_ws3/5E703E3A703E18EB/data/subt_all/\", mode='val'),\n",
    "                            batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA:  True\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'test1'\n",
    "root = '/media/arg_ws3/5E703E3A703E18EB/research/InstaceGAN/'\n",
    "os.makedirs(root + 'images/%s' % dataset_name, exist_ok=True)\n",
    "os.makedirs(root + 'saved_models/%s' % dataset_name, exist_ok=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "print(\"CUDA: \", cuda)\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G = Generator()\n",
    "E = Encoder()\n",
    "D = Discriminator(3)\n",
    "\n",
    "if cuda:\n",
    "    G = G.cuda()\n",
    "    E = E.cuda()\n",
    "    D = D.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_pixelwise.cuda()\n",
    "    \n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_E = torch.optim.Adam(E.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def predict():\n",
    "    transforms_ = [ transforms.Resize((128, 128), Image.BICUBIC),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\n",
    "    data_transform = transforms.Compose(transforms_)\n",
    "    prev_time = time.time()\n",
    "    rgb_path = \"/media/arg_ws3/5E703E3A703E18EB/20190929_183837.jpg\"\n",
    "    fff = cv2.imread(rgb_path)\n",
    "    image = Image.open(rgb_path).resize((128, 128), Image.ANTIALIAS)\n",
    "    #image = cv2.resize(image, (128, 128))\n",
    "    #pil_im = Image.fromarray(image)\n",
    "    pil_im = data_transform(image).unsqueeze(dim=0)\n",
    "\n",
    "    my_img = Variable(pil_im.type(Tensor))\n",
    "    y = E(my_img)\n",
    "    r_label = torch.tensor([3])\n",
    "    r_label = r_label.unsqueeze(0).cpu()\n",
    "    r_label = one_hot(r_label, [0, 1]).cuda()\n",
    "    z = r_label.unsqueeze(-1).unsqueeze(-1)\n",
    "    z = torch.cat((y, z), 1)\n",
    "    my_img_fake = G(z)\n",
    "    my_img_fake = my_img_fake.squeeze(0).detach().cpu()\n",
    "\n",
    "    pil_ = my_img_fake.mul(255).clamp(0, 255).byte().permute(1, 2, 0)\n",
    "    pil_ = np.array(pil_)\n",
    "    pil_ = pil_[...,::-1]\n",
    "    pil_ = cv2.resize(pil_, (640, 480))\n",
    "    pil_ = pil_[:,:,0]\n",
    "    fff = cv2.resize(fff, (128, 128))\n",
    "    fff = cv2.resize(fff, (640, 480))\n",
    "    cv2.imwrite(\"crop.jpg\", fff)\n",
    "    cv2.imwrite(\"crop_mask.jpg\", pil_)\n",
    "    print(\"Hz: \", 1./(time.time() - prev_time))\n",
    "    save_image(my_img_fake.data, 'crop_mask.png', nrow=1, normalize=True)\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    r_rgb = Variable(imgs['B'].type(Tensor)) # rgb\n",
    "    r_mask = Variable(imgs['A'].type(Tensor)) # mask\n",
    "    r_label = Variable(imgs['C'].type(Tensor)) # label\n",
    "    \n",
    "    y = E(r_rgb)\n",
    "    \n",
    "    f_mask = G(y, r_label)\n",
    "    \n",
    "    f_mask = f_mask.data.cpu().numpy()\n",
    "    N, _, h, w = f_mask.shape\n",
    "    f_mask = f_mask.transpose(0, 2, 3, 1).reshape(-1, 2).argmax(axis = 1).reshape(N, h, w)\n",
    "    \n",
    "    #fake_B_d = f_mask[:,1,:,:].unsqueeze(1).data\n",
    "    fake_B_d = torch.tensor(f_mask, dtype=torch.float).unsqueeze(1).cuda()\n",
    "    fake_B_d = torch.cat((fake_B_d, fake_B_d, fake_B_d), 1)\n",
    "    real_B_d = r_mask[:,1,:,:].unsqueeze(1).data\n",
    "    real_B_d = torch.cat((real_B_d, real_B_d, real_B_d), 1)\n",
    "    img_sample = torch.cat((r_rgb.data, fake_B_d, real_B_d), 0)\n",
    "    save_image(img_sample, root + 'images/%s/%s.png' % (dataset_name, batches_done), nrow=5, normalize=False)\n",
    "\n",
    "sample_images(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 57.224880] ETA: 6:37:38.682275\n",
      "[Epoch 0/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 36.090855] ETA: 1:17:14.114325\n",
      "[Epoch 0/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 38.851040] ETA: 1:19:54.867337\n",
      "[Epoch 0/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 33.046883] ETA: 1:19:38.207839\n",
      "[Epoch 0/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 32.442081] ETA: 1:19:27.981350\n",
      "[Epoch 0/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 32.264893] ETA: 1:20:30.201924\n",
      "[Epoch 0/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 32.167072] ETA: 1:18:58.127410\n",
      "[Epoch 1/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 32.698483] ETA: 7:38:37.803183\n",
      "[Epoch 1/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 32.411083] ETA: 1:18:57.831686\n",
      "[Epoch 1/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 31.828892] ETA: 1:18:12.511849\n",
      "[Epoch 1/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 32.739037] ETA: 1:19:36.878446\n",
      "[Epoch 1/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 32.191784] ETA: 1:24:36.732240\n",
      "[Epoch 1/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 33.793812] ETA: 1:19:28.128086\n",
      "[Epoch 1/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 32.879246] ETA: 1:16:57.758106\n",
      "[Epoch 2/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 32.327290] ETA: 6:46:04.628277\n",
      "[Epoch 2/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 34.495228] ETA: 1:17:37.904849\n",
      "[Epoch 2/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 31.560648] ETA: 1:20:38.107138\n",
      "[Epoch 2/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 32.581276] ETA: 1:19:16.111836\n",
      "[Epoch 2/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 31.312874] ETA: 1:15:55.518913\n",
      "[Epoch 2/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 31.711197] ETA: 1:15:46.896963\n",
      "[Epoch 2/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 31.717834] ETA: 1:16:33.132362\n",
      "[Epoch 3/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 31.517374] ETA: 1 day, 4:46:29.511634\n",
      "[Epoch 3/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 31.389111] ETA: 1:14:30.079486\n",
      "[Epoch 3/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 32.278999] ETA: 1:14:48.678305\n",
      "[Epoch 3/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 34.935276] ETA: 1:15:47.969184\n",
      "[Epoch 3/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 30.280458] ETA: 1:13:59.579959\n",
      "[Epoch 3/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 31.093424] ETA: 1:15:35.131719\n",
      "[Epoch 3/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 31.439554] ETA: 1:15:23.133838\n",
      "[Epoch 4/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 31.357544] ETA: 6:27:20.765841\n",
      "[Epoch 4/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 32.617344] ETA: 1:18:29.319725\n",
      "[Epoch 4/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 31.708389] ETA: 1:13:33.983624\n",
      "[Epoch 4/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 32.364601] ETA: 1:13:58.674760\n",
      "[Epoch 4/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 32.401806] ETA: 1:14:20.129118\n",
      "[Epoch 4/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 33.904526] ETA: 1:13:48.913591\n",
      "[Epoch 4/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 31.893307] ETA: 1:14:04.324894\n",
      "[Epoch 5/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 30.877935] ETA: 1 day, 3:59:09.533433\n",
      "[Epoch 5/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 32.517139] ETA: 1:14:14.993796\n",
      "[Epoch 5/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 30.103418] ETA: 1:18:06.912656\n",
      "[Epoch 5/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 33.512287] ETA: 1:12:17.559056\n",
      "[Epoch 5/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 34.958893] ETA: 1:12:48.225306\n",
      "[Epoch 5/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 31.757349] ETA: 1:10:47.736430\n",
      "[Epoch 5/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 30.298948] ETA: 1:11:04.841527\n",
      "[Epoch 6/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 31.930515] ETA: 6:10:57.016225\n",
      "[Epoch 6/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 30.420300] ETA: 1:09:48.168707\n",
      "[Epoch 6/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 30.342651] ETA: 1:11:21.050186\n",
      "[Epoch 6/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 30.161905] ETA: 1:10:25.682030\n",
      "[Epoch 6/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 30.668360] ETA: 1:12:21.131244\n",
      "[Epoch 6/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 31.401592] ETA: 1:12:31.736555\n",
      "[Epoch 6/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 31.182373] ETA: 1:08:44.558873\n",
      "[Epoch 7/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 31.229776] ETA: 1 day, 3:18:10.997998\n",
      "[Epoch 7/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 31.098228] ETA: 1:09:19.772010\n",
      "[Epoch 7/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 30.219549] ETA: 1:08:40.798465\n",
      "[Epoch 7/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 32.064648] ETA: 1:07:57.987063\n",
      "[Epoch 7/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 31.717373] ETA: 1:08:15.490218\n",
      "[Epoch 7/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 30.964289] ETA: 1:07:33.689463\n",
      "[Epoch 7/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 31.816460] ETA: 1:07:10.647326\n",
      "[Epoch 8/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 30.615013] ETA: 6:10:36.443202\n",
      "[Epoch 8/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 32.479824] ETA: 1:08:26.905754\n",
      "[Epoch 8/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 31.383032] ETA: 1:07:06.067522\n",
      "[Epoch 8/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 30.196930] ETA: 1:08:21.670675\n",
      "[Epoch 8/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 30.599594] ETA: 1:07:16.143909\n",
      "[Epoch 8/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 29.602592] ETA: 1:08:52.248998\n",
      "[Epoch 8/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.718182] ETA: 1:07:24.306650\n",
      "[Epoch 9/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.209633] ETA: 1 day, 1:00:30.111208\n",
      "[Epoch 9/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 29.510902] ETA: 1:04:55.088845\n",
      "[Epoch 9/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.891916] ETA: 1:04:58.007866\n",
      "[Epoch 9/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.572079] ETA: 1:05:16.510899\n",
      "[Epoch 9/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.757120] ETA: 1:04:58.656981\n",
      "[Epoch 9/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 29.193720] ETA: 1:07:46.924709\n",
      "[Epoch 9/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.370611] ETA: 1:04:28.989412\n",
      "[Epoch 10/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.315140] ETA: 5:41:54.019632\n",
      "[Epoch 10/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 29.433941] ETA: 1:03:50.157399\n",
      "[Epoch 10/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.454870] ETA: 1:05:09.137154\n",
      "[Epoch 10/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.992208] ETA: 1:03:40.815825\n",
      "[Epoch 10/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.627295] ETA: 1:04:07.363758\n",
      "[Epoch 10/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 29.724670] ETA: 1:02:41.115789\n",
      "[Epoch 10/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.198467] ETA: 1:02:42.706661\n",
      "[Epoch 11/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 29.158789] ETA: 23:48:23.398079\n",
      "[Epoch 11/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 29.422224] ETA: 1:03:15.714450\n",
      "[Epoch 11/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 29.548105] ETA: 1:02:10.056063\n",
      "[Epoch 11/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.764086] ETA: 1:02:21.198045\n",
      "[Epoch 11/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 29.359978] ETA: 1:08:26.646620\n",
      "[Epoch 11/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.176924] ETA: 1:02:47.998701\n",
      "[Epoch 11/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.076111] ETA: 1:01:33.938102\n",
      "[Epoch 12/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 29.041771] ETA: 5:22:49.827719\n",
      "[Epoch 12/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.859564] ETA: 1:01:49.636455\n",
      "[Epoch 12/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.162109] ETA: 1:00:46.054575\n",
      "[Epoch 12/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.121832] ETA: 1:00:31.641998\n",
      "[Epoch 12/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.746922] ETA: 1:00:37.307076\n",
      "[Epoch 12/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.436644] ETA: 1:01:05.566435\n",
      "[Epoch 12/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 26.985842] ETA: 1:02:08.536098\n",
      "[Epoch 13/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.055021] ETA: 22:05:39.943990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.264120] ETA: 0:59:07.940413\n",
      "[Epoch 13/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.014576] ETA: 0:58:47.307376\n",
      "[Epoch 13/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 26.628527] ETA: 0:59:27.391287\n",
      "[Epoch 13/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 26.947868] ETA: 1:00:01.748548\n",
      "[Epoch 13/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.153154] ETA: 0:59:22.875806\n",
      "[Epoch 13/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.096348] ETA: 0:58:56.104932\n",
      "[Epoch 14/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 29.339569] ETA: 5:12:41.739120\n",
      "[Epoch 14/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.914993] ETA: 0:58:27.930222\n",
      "[Epoch 14/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.159119] ETA: 0:57:46.170230\n",
      "[Epoch 14/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.475838] ETA: 0:58:19.345207\n",
      "[Epoch 14/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.299934] ETA: 0:56:57.479396\n",
      "[Epoch 14/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 29.264141] ETA: 0:58:44.668007\n",
      "[Epoch 14/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.922745] ETA: 0:56:55.680285\n",
      "[Epoch 15/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.662533] ETA: 21:32:39.174013\n",
      "[Epoch 15/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.832275] ETA: 0:57:18.269770\n",
      "[Epoch 15/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.881777] ETA: 0:56:36.538359\n",
      "[Epoch 15/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.629791] ETA: 0:55:51.249951\n",
      "[Epoch 15/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.169477] ETA: 0:55:40.642291\n",
      "[Epoch 15/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 29.047371] ETA: 0:56:06.055101\n",
      "[Epoch 15/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.386070] ETA: 0:54:46.511528\n",
      "[Epoch 16/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.724617] ETA: 4:52:18.822234\n",
      "[Epoch 16/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.071798] ETA: 0:54:32.947035\n",
      "[Epoch 16/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.398914] ETA: 0:55:48.282881\n",
      "[Epoch 16/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.211527] ETA: 0:54:13.053446\n",
      "[Epoch 16/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 30.085638] ETA: 0:53:42.680075\n",
      "[Epoch 16/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.943996] ETA: 0:55:07.195666\n",
      "[Epoch 16/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.950829] ETA: 0:53:16.773584\n",
      "[Epoch 17/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.807421] ETA: 19:51:16.561300\n",
      "[Epoch 17/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.468575] ETA: 0:54:08.623925\n",
      "[Epoch 17/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.280622] ETA: 0:54:10.804522\n",
      "[Epoch 17/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.022116] ETA: 0:52:58.031813\n",
      "[Epoch 17/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.854519] ETA: 0:52:26.621742\n",
      "[Epoch 17/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 30.392456] ETA: 0:52:17.552602\n",
      "[Epoch 17/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.931067] ETA: 0:52:38.761071\n",
      "[Epoch 18/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.183533] ETA: 4:30:15.808525\n",
      "[Epoch 18/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 29.485649] ETA: 0:51:23.955779\n",
      "[Epoch 18/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 29.336088] ETA: 0:51:36.508017\n",
      "[Epoch 18/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.328121] ETA: 0:50:58.749919\n",
      "[Epoch 18/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.431208] ETA: 0:50:39.468632\n",
      "[Epoch 18/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.298407] ETA: 0:50:35.895271\n",
      "[Epoch 18/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.216730] ETA: 0:50:46.116571\n",
      "[Epoch 19/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.675776] ETA: 19:10:14.276090\n",
      "[Epoch 19/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.588213] ETA: 0:49:35.766512\n",
      "[Epoch 19/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.885283] ETA: 0:49:47.877573\n",
      "[Epoch 19/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.323029] ETA: 0:49:47.655530\n",
      "[Epoch 19/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.725180] ETA: 0:49:08.313684\n",
      "[Epoch 19/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.508301] ETA: 0:50:30.418047\n",
      "[Epoch 19/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.152519] ETA: 0:48:44.170521\n",
      "[Epoch 20/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.363422] ETA: 4:18:09.362133\n",
      "[Epoch 20/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.957642] ETA: 0:48:00.213881\n",
      "[Epoch 20/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.735931] ETA: 0:48:05.387635\n",
      "[Epoch 20/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.046642] ETA: 0:47:31.063371\n",
      "[Epoch 20/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.811789] ETA: 0:47:54.387980\n",
      "[Epoch 20/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.265821] ETA: 0:46:51.404097\n",
      "[Epoch 20/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 30.285978] ETA: 0:47:24.812500\n",
      "[Epoch 21/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.957664] ETA: 17:53:27.702638\n",
      "[Epoch 21/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.648100] ETA: 0:47:56.538150\n",
      "[Epoch 21/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 29.152180] ETA: 0:46:09.264244\n",
      "[Epoch 21/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 26.885033] ETA: 0:46:01.752584\n",
      "[Epoch 21/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.724749] ETA: 0:47:46.502970\n",
      "[Epoch 21/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.570137] ETA: 0:45:28.143762\n",
      "[Epoch 21/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.068417] ETA: 0:44:54.544744\n",
      "[Epoch 22/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.787249] ETA: 3:56:57.382379\n",
      "[Epoch 22/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.883106] ETA: 0:46:38.707800\n",
      "[Epoch 22/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.964323] ETA: 0:45:34.964442\n",
      "[Epoch 22/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.448923] ETA: 0:44:41.271343\n",
      "[Epoch 22/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.551340] ETA: 0:44:10.205436\n",
      "[Epoch 22/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.638184] ETA: 0:44:11.613941\n",
      "[Epoch 22/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.201220] ETA: 0:43:43.925900\n",
      "[Epoch 23/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.255728] ETA: 16:42:37.079329\n",
      "[Epoch 23/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.688688] ETA: 0:43:35.496738\n",
      "[Epoch 23/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.920033] ETA: 0:44:21.791505\n",
      "[Epoch 23/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.964222] ETA: 0:43:28.604068\n",
      "[Epoch 23/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 26.958012] ETA: 0:43:11.201069\n",
      "[Epoch 23/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.924171] ETA: 0:42:06.046445\n",
      "[Epoch 23/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.225737] ETA: 0:43:00.560757\n",
      "[Epoch 24/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.648426] ETA: 3:40:01.686208\n",
      "[Epoch 24/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.761896] ETA: 0:42:19.420562\n",
      "[Epoch 24/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.624023] ETA: 0:42:29.392555\n",
      "[Epoch 24/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.335684] ETA: 0:44:09.346461\n",
      "[Epoch 24/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.746746] ETA: 0:41:07.019668\n",
      "[Epoch 24/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.507416] ETA: 0:41:26.197557\n",
      "[Epoch 24/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.233892] ETA: 0:40:53.701093\n",
      "[Epoch 25/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.744438] ETA: 15:10:09.939963\n",
      "[Epoch 25/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.836388] ETA: 0:40:55.177873\n",
      "[Epoch 25/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.648058] ETA: 0:39:30.311826\n",
      "[Epoch 25/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.452206] ETA: 0:40:19.107467\n",
      "[Epoch 25/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 30.213394] ETA: 0:39:14.997993\n",
      "[Epoch 25/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.859482] ETA: 0:39:08.081946\n",
      "[Epoch 25/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.032539] ETA: 0:40:33.672577\n",
      "[Epoch 26/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.845114] ETA: 4:19:36.000681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.279284] ETA: 0:38:18.587904\n",
      "[Epoch 26/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 29.080725] ETA: 0:38:14.060097\n",
      "[Epoch 26/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.016769] ETA: 0:38:07.997761\n",
      "[Epoch 26/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 26.881742] ETA: 0:38:43.230515\n",
      "[Epoch 26/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.785700] ETA: 0:40:25.006447\n",
      "[Epoch 26/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.328199] ETA: 0:37:09.938049\n",
      "[Epoch 27/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.030176] ETA: 15:25:02.116005\n",
      "[Epoch 27/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.343899] ETA: 0:38:12.611792\n",
      "[Epoch 27/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 26.711241] ETA: 0:36:48.368527\n",
      "[Epoch 27/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.961622] ETA: 0:36:30.460700\n",
      "[Epoch 27/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.840330] ETA: 0:36:22.698257\n",
      "[Epoch 27/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.975786] ETA: 0:37:17.790302\n",
      "[Epoch 27/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.582731] ETA: 0:35:45.729361\n",
      "[Epoch 28/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.408075] ETA: 3:18:09.484255\n",
      "[Epoch 28/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 29.200554] ETA: 0:34:48.881838\n",
      "[Epoch 28/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.304966] ETA: 0:35:04.537797\n",
      "[Epoch 28/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.080446] ETA: 0:35:04.792542\n",
      "[Epoch 28/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.012651] ETA: 0:34:24.660153\n",
      "[Epoch 28/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.481216] ETA: 0:36:06.911595\n",
      "[Epoch 28/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.818663] ETA: 0:34:24.665236\n",
      "[Epoch 29/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.175949] ETA: 12:47:40.935878\n",
      "[Epoch 29/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.486217] ETA: 0:34:19.909916\n",
      "[Epoch 29/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 29.778814] ETA: 0:33:30.255443\n",
      "[Epoch 29/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.398149] ETA: 0:34:03.450394\n",
      "[Epoch 29/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.936836] ETA: 0:33:01.378635\n",
      "[Epoch 29/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 30.414742] ETA: 0:32:45.989945\n",
      "[Epoch 29/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.297297] ETA: 0:32:44.516973\n",
      "[Epoch 30/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.713865] ETA: 2:55:58.420587\n",
      "[Epoch 30/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.100739] ETA: 0:31:57.837715\n",
      "[Epoch 30/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 26.945719] ETA: 0:31:28.070583\n",
      "[Epoch 30/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.541143] ETA: 0:32:07.861977\n",
      "[Epoch 30/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.262865] ETA: 0:31:26.014342\n",
      "[Epoch 30/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.647640] ETA: 0:31:07.118454\n",
      "[Epoch 30/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.362745] ETA: 0:31:51.991525\n",
      "[Epoch 31/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.021711] ETA: 11:31:15.157886\n",
      "[Epoch 31/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.402889] ETA: 0:31:05.530052\n",
      "[Epoch 31/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 26.780165] ETA: 0:30:03.892677\n",
      "[Epoch 31/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.304741] ETA: 0:32:17.357397\n",
      "[Epoch 31/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.422695] ETA: 0:29:52.417892\n",
      "[Epoch 31/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.621841] ETA: 0:30:02.486281\n",
      "[Epoch 31/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.024429] ETA: 0:29:11.575613\n",
      "[Epoch 32/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 26.982616] ETA: 2:33:53.792989\n",
      "[Epoch 32/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.572771] ETA: 0:31:19.860439\n",
      "[Epoch 32/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.638882] ETA: 0:29:06.112764\n",
      "[Epoch 32/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 26.889961] ETA: 0:28:57.205539\n",
      "[Epoch 32/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 26.671642] ETA: 0:28:07.295978\n",
      "[Epoch 32/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.039173] ETA: 0:27:50.347574\n",
      "[Epoch 32/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.263712] ETA: 0:27:53.721757\n",
      "[Epoch 33/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.323132] ETA: 10:33:21.825771\n",
      "[Epoch 33/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.456671] ETA: 0:27:06.159893\n",
      "[Epoch 33/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.058445] ETA: 0:28:01.213733\n",
      "[Epoch 33/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.427008] ETA: 0:27:26.298469\n",
      "[Epoch 33/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.962893] ETA: 0:26:42.107089\n",
      "[Epoch 33/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.320503] ETA: 0:26:26.769559\n",
      "[Epoch 33/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.103434] ETA: 0:26:18.097637\n",
      "[Epoch 34/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.531973] ETA: 2:20:51.100788\n",
      "[Epoch 34/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.440252] ETA: 0:26:07.159796\n",
      "[Epoch 34/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.926548] ETA: 0:25:41.588888\n",
      "[Epoch 34/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.174881] ETA: 0:24:57.896504\n",
      "[Epoch 34/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.077396] ETA: 0:24:55.258789\n",
      "[Epoch 34/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.880301] ETA: 0:24:47.481947\n",
      "[Epoch 34/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 26.914125] ETA: 0:24:29.737072\n",
      "[Epoch 35/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 29.328178] ETA: 8:57:20.114701\n",
      "[Epoch 35/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.762484] ETA: 0:24:01.849756\n",
      "[Epoch 35/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.274900] ETA: 0:24:48.387609\n",
      "[Epoch 35/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.217228] ETA: 0:23:47.465039\n",
      "[Epoch 35/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 26.707893] ETA: 0:23:34.004046\n",
      "[Epoch 35/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.805017] ETA: 0:23:11.316938\n",
      "[Epoch 35/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.499792] ETA: 0:22:45.923345\n",
      "[Epoch 36/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 26.949560] ETA: 2:03:49.616375\n",
      "[Epoch 36/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.009142] ETA: 0:22:15.869141\n",
      "[Epoch 36/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.846645] ETA: 0:22:17.197762\n",
      "[Epoch 36/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.152180] ETA: 0:22:10.081766\n",
      "[Epoch 36/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.623571] ETA: 0:22:11.770904\n",
      "[Epoch 36/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.166616] ETA: 0:21:48.478117\n",
      "[Epoch 36/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.734196] ETA: 0:21:12.555420\n",
      "[Epoch 37/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.900780] ETA: 7:51:18.409138\n",
      "[Epoch 37/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 26.570839] ETA: 0:20:32.656124\n",
      "[Epoch 37/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.967184] ETA: 0:21:29.345902\n",
      "[Epoch 37/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.505783] ETA: 0:20:31.459125\n",
      "[Epoch 37/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.431334] ETA: 0:20:18.713735\n",
      "[Epoch 37/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.214539] ETA: 0:20:21.492898\n",
      "[Epoch 37/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.812963] ETA: 0:20:04.380394\n",
      "[Epoch 38/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.743441] ETA: 1:45:39.309511\n",
      "[Epoch 38/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.031153] ETA: 0:19:13.967924\n",
      "[Epoch 38/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.131346] ETA: 0:18:46.044164\n",
      "[Epoch 38/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.266144] ETA: 0:18:35.991898\n",
      "[Epoch 38/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 29.573658] ETA: 0:18:51.092401\n",
      "[Epoch 38/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.707949] ETA: 0:18:13.775234\n",
      "[Epoch 38/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.788900] ETA: 0:18:01.215506\n",
      "[Epoch 39/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.814098] ETA: 6:44:49.610864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 26.991034] ETA: 0:18:11.326991\n",
      "[Epoch 39/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 26.512754] ETA: 0:17:15.528567\n",
      "[Epoch 39/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 29.509199] ETA: 0:17:00.014166\n",
      "[Epoch 39/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.034027] ETA: 0:16:44.198117\n",
      "[Epoch 39/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.653898] ETA: 0:16:35.478581\n",
      "[Epoch 39/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.139019] ETA: 0:16:52.340051\n",
      "[Epoch 40/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.259836] ETA: 1:30:40.794003\n",
      "[Epoch 40/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.470612] ETA: 0:16:16.167047\n",
      "[Epoch 40/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.027111] ETA: 0:15:40.722048\n",
      "[Epoch 40/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.685541] ETA: 0:15:46.023488\n",
      "[Epoch 40/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.177256] ETA: 0:15:51.886308\n",
      "[Epoch 40/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 30.271172] ETA: 0:15:09.648299\n",
      "[Epoch 40/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.112782] ETA: 0:14:55.180500\n",
      "[Epoch 41/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.648142] ETA: 6:03:38.425723\n",
      "[Epoch 41/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.785641] ETA: 0:14:17.050648\n",
      "[Epoch 41/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.393797] ETA: 0:14:04.199287\n",
      "[Epoch 41/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 26.901781] ETA: 0:14:16.171342\n",
      "[Epoch 41/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.789669] ETA: 0:13:35.749651\n",
      "[Epoch 41/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.841644] ETA: 0:13:34.750102\n",
      "[Epoch 41/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 26.765141] ETA: 0:13:26.412245\n",
      "[Epoch 42/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.053152] ETA: 1:08:33.104239\n",
      "[Epoch 42/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.475058] ETA: 0:12:46.102214\n",
      "[Epoch 42/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.958315] ETA: 0:12:25.858917\n",
      "[Epoch 42/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.662188] ETA: 0:12:15.359488\n",
      "[Epoch 42/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.496372] ETA: 0:12:11.726418\n",
      "[Epoch 42/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.641052] ETA: 0:12:00.021024\n",
      "[Epoch 42/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.369389] ETA: 0:11:33.205662\n",
      "[Epoch 43/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.253109] ETA: 4:16:59.524913\n",
      "[Epoch 43/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.178474] ETA: 0:11:11.625423\n",
      "[Epoch 43/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 26.796473] ETA: 0:10:50.891882\n",
      "[Epoch 43/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.683311] ETA: 0:10:36.698391\n",
      "[Epoch 43/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.511528] ETA: 0:10:25.198919\n",
      "[Epoch 43/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.287500] ETA: 0:10:04.463114\n",
      "[Epoch 43/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 26.385481] ETA: 0:09:50.366660\n",
      "[Epoch 44/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.898052] ETA: 0:52:53.156927\n",
      "[Epoch 44/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.425171] ETA: 0:09:34.430871\n",
      "[Epoch 44/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.640257] ETA: 0:09:31.174655\n",
      "[Epoch 44/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.360981] ETA: 0:09:38.221292\n",
      "[Epoch 44/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 27.985882] ETA: 0:08:45.906796\n",
      "[Epoch 44/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.512810] ETA: 0:08:39.804068\n",
      "[Epoch 44/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 28.879511] ETA: 0:08:33.395562\n",
      "[Epoch 45/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.941614] ETA: 3:03:37.270750\n",
      "[Epoch 45/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.305805] ETA: 0:07:47.039716\n",
      "[Epoch 45/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 26.898701] ETA: 0:07:48.294197\n",
      "[Epoch 45/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 26.874718] ETA: 0:07:23.285465\n",
      "[Epoch 45/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.925694] ETA: 0:07:11.559515\n",
      "[Epoch 45/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.842012] ETA: 0:06:57.101169\n",
      "[Epoch 45/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 26.890606] ETA: 0:06:40.743520\n",
      "[Epoch 46/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.024990] ETA: 0:33:59.286432\n",
      "[Epoch 46/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.321070] ETA: 0:06:11.024475\n",
      "[Epoch 46/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.551609] ETA: 0:06:01.268706\n",
      "[Epoch 46/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 26.964033] ETA: 0:05:47.342577\n",
      "[Epoch 46/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.471571] ETA: 0:05:32.758384\n",
      "[Epoch 46/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.973974] ETA: 0:05:19.142609\n",
      "[Epoch 46/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.266306] ETA: 0:05:06.686611\n",
      "[Epoch 47/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 26.787354] ETA: 1:49:36.441561\n",
      "[Epoch 47/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.031523] ETA: 0:04:41.888298\n",
      "[Epoch 47/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 26.990610] ETA: 0:04:24.291388\n",
      "[Epoch 47/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.318712] ETA: 0:04:09.762025\n",
      "[Epoch 47/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.403599] ETA: 0:03:56.603839\n",
      "[Epoch 47/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 26.522345] ETA: 0:03:43.794199\n",
      "[Epoch 47/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.819092] ETA: 0:03:27.941734\n",
      "[Epoch 48/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 27.445110] ETA: 0:18:36.791525\n",
      "[Epoch 48/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 27.992599] ETA: 0:03:00.578690\n",
      "[Epoch 48/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 28.126493] ETA: 0:02:44.481452\n",
      "[Epoch 48/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 28.661209] ETA: 0:02:30.791044\n",
      "[Epoch 48/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.935797] ETA: 0:02:16.448915\n",
      "[Epoch 48/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 27.001154] ETA: 0:02:03.205276\n",
      "[Epoch 48/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 29.163689] ETA: 0:01:48.287358\n",
      "[Epoch 49/50] [Batch 0/3405] [D loss: 0.500000] [G loss: 28.251129] ETA: 0:36:40.169545\n",
      "[Epoch 49/50] [Batch 500/3405] [D loss: 0.500000] [G loss: 28.493530] ETA: 0:01:22.286438\n",
      "[Epoch 49/50] [Batch 1000/3405] [D loss: 0.500000] [G loss: 27.146669] ETA: 0:01:09.448659\n",
      "[Epoch 49/50] [Batch 1500/3405] [D loss: 0.500000] [G loss: 27.038816] ETA: 0:00:54.070555\n",
      "[Epoch 49/50] [Batch 2000/3405] [D loss: 0.500000] [G loss: 28.081926] ETA: 0:00:39.988011\n",
      "[Epoch 49/50] [Batch 2500/3405] [D loss: 0.500000] [G loss: 28.285450] ETA: 0:00:26.172973\n",
      "[Epoch 49/50] [Batch 3000/3405] [D loss: 0.500000] [G loss: 27.116255] ETA: 0:00:11.957352\n"
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Model inputs\n",
    "        r_rgb = Variable(batch['B'].type(Tensor)) # rgb\n",
    "        r_mask = Variable(batch['A'].type(Tensor)) # mask\n",
    "        r_label = Variable(batch['C'].type(Tensor)) # label\n",
    "        #print(r_rgb.shape, r_mask.shape, r_label.shape)\n",
    "        # Adversarial ground truths\n",
    "        #valid = Variable(Tensor(np.ones((real_B.size(0), *patch))), requires_grad=False)\n",
    "        #fake = Variable(Tensor(np.zeros((real_B.size(0), *patch))), requires_grad=False)\n",
    "        valid = Variable(Tensor(np.ones(1)), requires_grad=False) #1\n",
    "        fake = Variable(Tensor(np.zeros(1)), requires_grad=False) #0\n",
    "        \n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_E.zero_grad()\n",
    "\n",
    "        y = E(r_rgb)\n",
    "        # GAN loss\n",
    "        f_mask = G(y, r_label)\n",
    "        f_pred = D(f_mask, r_rgb, r_label)\n",
    "        loss_GAN = criterion_GAN(f_pred, valid)\n",
    "        # Pixel-wise loss\n",
    "        \n",
    "        loss_pixel = criterion_pixelwise(f_mask, r_mask)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "        #loss_G = loss_GAN\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_E.step()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        r_pred = D(r_mask, r_rgb, r_label)\n",
    "        loss_real = criterion_GAN(r_pred, valid)\n",
    "\n",
    "        # Fake loss\n",
    "        f_pred = D(f_mask.detach(), r_rgb, r_label)\n",
    "        loss_fake = criterion_GAN(f_pred, fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        if i % 1000 == 0:\n",
    "            sample_images(batches_done)\n",
    "        if i % 500 == 0:\n",
    "            print(\"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] ETA: %s\" %\n",
    "                                (epoch, n_epochs,\n",
    "                                i, len(dataloader),\n",
    "                                loss_D.item(), loss_G.item(),\n",
    "                                time_left))\n",
    "    if epoch % 2 == 0 and epoch != 0:\n",
    "        torch.save(G.state_dict(), root + 'saved_models/%s/G_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(E.state_dict(), root + 'saved_models/%s/E_%d.pth' % (dataset_name, epoch))\n",
    "        torch.save(D.state_dict(), root + 'saved_models/%s/D_%d.pth' % (dataset_name, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
