{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision.models.vgg import VGG\n",
    "from torchvision import models\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        layers = [  nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "                    nn.InstanceNorm2d(out_size),\n",
    "                    nn.ReLU(inplace=True)]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=2):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cls):\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B, cls):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        print(img_input.shape)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN16s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace = True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        print(output['x5'].shape)\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
    "        super().__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
    "            del self.classifier\n",
    "\n",
    "        if show_params:\n",
    "            for name, param in self.named_parameters():\n",
    "                print(name, param.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):      \n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
    "    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
    "    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
    "    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
    "}\n",
    "\n",
    "# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 10\n",
    "vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n",
    "fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 256, 256])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = fcn_model(item)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 352, 480)\n",
      "(1, 352, 480, 10)\n",
      "(168960, 10)\n",
      "(168960,)\n",
      "(1, 352, 480)\n",
      "[6 9 6 ... 3 0 0]\n"
     ]
    }
   ],
   "source": [
    "output = fcn_model(item)\n",
    "output = output.data.cpu().numpy()\n",
    "N, _, h, w = output.shape\n",
    "print(output.shape)\n",
    "a1 = output.transpose(0, 2, 3, 1)\n",
    "a2 = a1.reshape(-1, n_class)\n",
    "a3 = a2.argmax(axis=1)\n",
    "a4 = a3.reshape(N, h, w)\n",
    "print(a1.shape)\n",
    "print(a2.shape)\n",
    "print(a3.shape)\n",
    "print(a4.shape)\n",
    "print(a3)\n",
    "#pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis=1).reshape(N, h, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 256, 256])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((item, item), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640)\n",
      "torch.Size([480, 640])\n"
     ]
    }
   ],
   "source": [
    "n_class = 2\n",
    "img_path = \"/media/arg_ws3/5E703E3A703E18EB/data/subt_real_622/mask/extinguisher/scene000002/6.png\"\n",
    "label = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "print(label.shape)\n",
    "print(torch.from_numpy(label).size())\n",
    "h, w = label.shape\n",
    "target = torch.zeros(n_class, h, w)\n",
    "\n",
    "label[label!=0] = 1\n",
    "label = torch.from_numpy(label.copy()).long()\n",
    "for i in range(n_class):\n",
    "    target[i][label == i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_mask = Image.open(img_path)\n",
    "img_mask = np.array(img_mask)\n",
    "img_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 480, 2)\n",
      "(3, 256, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb0ed345cf8>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "back = target[0].numpy()\n",
    "obj = target[1].numpy()\n",
    "orig = target.numpy().transpose(2, 1, 0)\n",
    "print(orig.shape)\n",
    "print(item[0].numpy().shape)\n",
    "plt.imshow(back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"/home/arg_ws3/Pictures/Wallpapers/NickWang.jpg\")\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "tfs = [ transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "                #transforms.RandomCrop((img_height, img_width)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "transforms_ = transforms.Compose(tfs)\n",
    "item = transforms_(img)\n",
    "item = Variable(item)\n",
    "\n",
    "item = item.unsqueeze(0)\n",
    "item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-871ae78b5d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'cls'"
     ]
    }
   ],
   "source": [
    "d = Discriminator(in_channels=6)\n",
    "\n",
    "out = d(item, item)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arg_ws3/.local/lib/python3.5/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "g = GeneratorUNet(in_channels=3, out_channels=2)\n",
    "\n",
    "g_out = g(item)\n",
    "print(item.shape, g_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
